{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4495e47c-7698-4e2d-ad85-9f62766858bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Computer vision - quality inspection of PCB\n",
    "\n",
    "<div style=\"float:right\">\n",
    "<img width=\"300px\" src=\"https://raw.githubusercontent.com/databricks-industry-solutions/cv-quality-inspection/main/images/PCB1.png\">\n",
    "</div>\n",
    "\n",
    "In this demo, we will show you how Databricks can help you build end to end a computer vision model. \n",
    "\n",
    "We will use the [Visual Anomaly (VisA)](https://registry.opendata.aws/visa/) detection dataset, and build a pipeline to detect anomalies in our PCB images. \n",
    "\n",
    "Computer vision is a field of study that has been advancing rapidly in recent years, thanks to the availability of large amounts of data, powerful GPUs, pre-trained deep learning models, transfer learning and higher level frameworks. However, training and serving a computer vision model can be very hard because of different challenges:\n",
    "- Data ingestion and preprocessing at scale\n",
    "- Data volumes that require multiple GPUs for training\n",
    "- Governance and production-readiness requirement MLOps pipelines for the the end-to-end model lifecycle.\n",
    "- Demanding SLAs for streaming or real-time inferences\n",
    "\n",
    "Databricks Lakehouse is designed to make this overall process simple, letting Data Scientists focus on the core use-case.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1549883858499596&notebook=%2F00-introduction-deep-learning-vision&demo_name=computer-vision-pcb&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fcomputer-vision-pcb%2F00-introduction-deep-learning-vision&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78aefa67-b04d-42cd-b81c-1515db8593eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Implementing our Deep Learning pipeline\n",
    "\n",
    "For building this model and easily solve those challenges, we will implement the following steps:\n",
    "\n",
    "- Ingestion pipeline with [Autoloader](https://docs.databricks.com/ingestion/auto-loader/index.html) to incrementally load and process our images into Delta Lake format.\n",
    "- Build a ML model with [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) using the Spark Dataset.\n",
    "- Run inference in batch or real-time using PandasUDF and [Databricks Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) for real-time inference.\n",
    "- Advanced: complete Torch version with [Spark Torch Distributor](https://docs.databricks.com/machine-learning/train-model/distributed-training/spark-pytorch-distributor.html) for distributed training.\n",
    "\n",
    "### Data flow\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/computer-vision/deeplearning-cv-pcb-flow-0.png?raw=true\" width=\"1100px\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca421799-979a-4674-a3ef-597657600021",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1/ Ingestion and ETL\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/computer-vision/deeplearning-cv-pcb-1.png?raw=true\" width=\"500px\" style=\"float: right\" />\n",
    "\n",
    "Our first step is to ingest the images. We will leverage Databricks Autoloader to ingest the images and the labels (as csv file).\n",
    "\n",
    "Our data will be saved as a Delta Table, allowing easy governance with Databricks Unity Catalog.\n",
    "\n",
    "We'll also apply initial transformations to reduce the image size and prepare our dataset for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddfc9b10-a49c-4291-801b-68d5ef03cc0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Open the [01-ingestion-and-ETL notebook]($./01-ingestion-and-ETL)  to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "304a947d-58ea-4c6d-a3d1-9d50085fad24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2/ Building our model with huggingface transformer\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/computer-vision/deeplearning-cv-pcb-2.png?raw=true\" width=\"500px\" style=\"float: right\" />\n",
    "\n",
    "Now that our data is ready, we can leverage huggingface transformer library and do fine-tuning in an existing state-of-the art model.\n",
    "\n",
    "This is a very efficient first approach to quickly deliver results without having to go into pytorch details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e5c517c-df55-4293-8bbb-776b9f4d6d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Open the [02-huggingface-model-training notebook]($./02-huggingface-model-training)  to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c4a4a40-ca07-4c27-8aaf-ee6555c2bc7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3/ Running inference in batch and deploying a realtime Serverless Model Endpoint\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/computer-vision/deeplearning-cv-pcb-3.png?raw=true\" width=\"500px\" style=\"float: right\" />\n",
    "\n",
    "Now that our model is created and available in our MLFlow registry, we'll be able to use it.\n",
    "\n",
    "Typical servinng use-case include:\n",
    "\n",
    "- Batch and Streaming use-cases (including with Delta Live Table)\n",
    "- Realtime model serving with Databricks Serverless Endpoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34196ae7-fb0a-4692-ae05-b4315d13b8e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Open the [03-running-cv-inferences notebook]($./03-running-cv-inferences) to run distributed and realtime inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3989e222-6cc6-4486-9893-b9c4b0af8611",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 4/ Explaining our model prediction and highlighting damanged PCB pixels\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/computer-vision/deeplearning-cv-pcb-explainer.png?raw=true\" width=\"500px\" style=\"float: right\" />\n",
    "\n",
    "Having inferences and score on each image is great, but our operators will need to know which part is considered as damaged for manual verification and potential fix.\n",
    "\n",
    "We will be using SHAP as explainer to highlight pixels having the most influence on the model prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79919f31-fbe4-42fa-af08-b0c0c00f704c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Open the [04-explaining-inference notebook]($./04-explaining-inference) to learn how to explain our inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeddd87a-a80f-48f2-a869-acb976a36a18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 4/ Going further with pytorch lightning and distributed workload\n",
    "\n",
    "More advanced use-cases might require leveraging other libraries such as pytorch or lightning. In this example, we will show how to implement a model fine tuning and inference with pytorch lightning.\n",
    "\n",
    "We will leverage delta-torch to easily build our torch dataset from the Delta tables.\n",
    "\n",
    "In addition, we'll also demonstrate how Databricks makes it easy to distribute the training on multiple GPUs using `TorchDistributor`.\n",
    "\n",
    "Open the [05-torch-lightning-training-and-inference]($./05-torch-lightning-training-and-inference) to see how to train your model & run distributed and realtime inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7afd5f6d-8e89-4326-b077-f609ea3772fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We covered how the Lakehouse is uniquely positioned to solve your ML and Deep Learning challenge:\n",
    "\n",
    "- Accelerate your data ingestion and transformation at scale\n",
    "- Simplify model training and governance\n",
    "- One-click model deployment for all use-cases, from batch to realtime inferences"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-introduction-deep-learning-vision",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
