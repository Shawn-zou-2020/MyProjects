{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "025bc617-24f6-4468-b1be-45f47a61c63a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "#Empty value will try default: dbdemos with a fallback to hive_metastore\n",
    "dbutils.widgets.text(\"catalog\", \"dbdemos\", \"Catalog\")\n",
    "dbutils.widgets.text(\"db\", \"manufacturing_pcb\", \"Database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c84de570-2480-4322-95c1-a3401a27efe7",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-global-setup $reset_all_data=$reset_all_data $catalog=$catalog $db=$db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f9c002d-2c58-43ee-bdf3-150a3c7319b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import mlflow\n",
    "\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "folder = \"/dbdemos/manufacturing/pcb/\"\n",
    "\n",
    "if reset_all_data:\n",
    "  dbutils.fs.rm(\"/dbdemos/manufacturing/pcb/\", True)\n",
    "if reset_all_data or is_folder_empty(folder+\"/labels\"):\n",
    "  #data generation on another notebook to avoid installing libraries (takes a few seconds to setup pip env)\n",
    "  print(f\"Loading raw data under {folder} , please wait a few minutes as we extract all images...\")\n",
    "  path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "  parent_count = path[path.rfind(\"computer-vision-dl\"):].count('/') - 1\n",
    "  prefix = \"./\" if parent_count == 0 else parent_count*\"../\"\n",
    "  prefix = f'{prefix}_resources/'\n",
    "  dbutils.notebook.run(prefix+\"01-load-data\", 600)\n",
    "else:\n",
    "  print(\"data already existing. Run with reset_all_data=true to force a data cleanup for your local demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e0fd04d-afaa-4d68-9ba5-bc7e03ab4303",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "class EndpointApiClient:\n",
    "    def __init__(self):\n",
    "        self.base_url =dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "        self.token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "        self.headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_inference_endpoint(self, endpoint_name, served_models):\n",
    "        data = {\"name\": endpoint_name, \"config\": {\"served_models\": served_models}}\n",
    "        return self._post(\"api/2.0/serving-endpoints\", data)\n",
    "\n",
    "    def get_inference_endpoint(self, endpoint_name):\n",
    "        return self._get(f\"api/2.0/serving-endpoints/{endpoint_name}\", allow_error=True)\n",
    "      \n",
    "      \n",
    "    def inference_endpoint_exists(self, endpoint_name):\n",
    "      ep = self.get_inference_endpoint(endpoint_name)\n",
    "      if 'error_code' in ep and ep['error_code'] == 'RESOURCE_DOES_NOT_EXIST':\n",
    "          return False\n",
    "      if 'error_code' in ep and ep['error_code'] != 'RESOURCE_DOES_NOT_EXIST':\n",
    "          raise Exception(f\"endpoint exists ? {ep}\")\n",
    "      return True\n",
    "\n",
    "    def create_endpoint_if_not_exists(self, endpoint_name, model_name, model_version, workload_size, scale_to_zero_enabled=True, wait_start=True):\n",
    "      models = [{\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version,\n",
    "            \"workload_size\": workload_size,\n",
    "            \"scale_to_zero_enabled\": scale_to_zero_enabled,\n",
    "      }]\n",
    "      if not self.inference_endpoint_exists(endpoint_name):\n",
    "        r = self.create_inference_endpoint(endpoint_name, models)\n",
    "      #Make sure we have the proper version deployed\n",
    "      else:\n",
    "        ep = self.get_inference_endpoint(endpoint_name)\n",
    "        if 'pending_config' in ep:\n",
    "            self.wait_endpoint_start(endpoint_name)\n",
    "            ep = self.get_inference_endpoint(endpoint_name)\n",
    "        if 'pending_config' in ep:\n",
    "            model_deployed = ep['pending_config']['served_models'][0]\n",
    "            print(f\"Error with the model deployed: {model_deployed} - state {ep['state']}\")\n",
    "        else:\n",
    "            model_deployed = ep['config']['served_models'][0]\n",
    "        if model_deployed['model_version'] != model_version:\n",
    "          print(f\"Current model is version {model_deployed['model_version']}. Updating to {model_version}...\")\n",
    "          u = self.update_model_endpoint(endpoint_name, {\"served_models\": models})\n",
    "      if wait_start:\n",
    "        self.wait_endpoint_start(endpoint_name)\n",
    "        time.sleep(10)\n",
    "      \n",
    "    def list_inference_endpoints(self):\n",
    "        return self._get(\"api/2.0/serving-endpoints\")\n",
    "\n",
    "    def update_model_endpoint(self, endpoint_name, conf):\n",
    "        return self._put(f\"api/2.0/serving-endpoints/{endpoint_name}/config\", conf)\n",
    "\n",
    "    def delete_inference_endpoint(self, endpoint_name):\n",
    "        return self._delete(f\"api/2.0/serving-endpoints/{endpoint_name}\")\n",
    "\n",
    "    def wait_endpoint_start(self, endpoint_name):\n",
    "      i = 0\n",
    "      while self.get_inference_endpoint(endpoint_name)['state']['config_update'] == \"IN_PROGRESS\" and i < 50:\n",
    "        print(\"waiting for endpoint to build model image and start\")\n",
    "        time.sleep(30)\n",
    "        i += 1\n",
    "      state = self.get_inference_endpoint(endpoint_name)\n",
    "      if state['state']['config_update'] == \"UPDATE_FAILED\":\n",
    "        print(f'WARN: ENDPOINT UPDATE FAILED: {state}')\n",
    "      else:\n",
    "        i = 0\n",
    "        while self.get_inference_endpoint(endpoint_name)['state']['ready'] != \"READY\" and i < 10:\n",
    "          print(\"Endpoint not ready...\")\n",
    "          time.sleep(10)\n",
    "          i += 1\n",
    "\n",
    "      \n",
    "    # Making predictions\n",
    "\n",
    "    def query_inference_endpoint(self, endpoint_name, data):\n",
    "        return self._post(f\"realtime-inference/{endpoint_name}/invocations\", data)\n",
    "\n",
    "    # Debugging\n",
    "\n",
    "    def get_served_model_build_logs(self, endpoint_name, served_model_name):\n",
    "        return self._get(\n",
    "            f\"api/2.0/serving-endpoints/{endpoint_name}/served-models/{served_model_name}/build-logs\"\n",
    "        )\n",
    "\n",
    "    def get_served_model_server_logs(self, endpoint_name, served_model_name):\n",
    "        return self._get(\n",
    "            f\"api/2.0/serving-endpoints/{endpoint_name}/served-models/{served_model_name}/logs\"\n",
    "        )\n",
    "\n",
    "    def get_inference_endpoint_events(self, endpoint_name):\n",
    "        return self._get(f\"api/2.0/serving-endpoints/{endpoint_name}/events\")\n",
    "\n",
    "    def _get(self, uri, data = {}, allow_error = False):\n",
    "        r = requests.get(f\"{self.base_url}/{uri}\", params=data, headers=self.headers)\n",
    "        return self._process(r, allow_error)\n",
    "\n",
    "    def _post(self, uri, data = {}, allow_error = False):\n",
    "        return self._process(requests.post(f\"{self.base_url}/{uri}\", json=data, headers=self.headers), allow_error)\n",
    "\n",
    "    def _put(self, uri, data = {}, allow_error = False):\n",
    "        return self._process(requests.put(f\"{self.base_url}/{uri}\", json=data, headers=self.headers), allow_error)\n",
    "\n",
    "    def _delete(self, uri, data = {}, allow_error = False):\n",
    "        return self._process(requests.delete(f\"{self.base_url}/{uri}\", json=data, headers=self.headers), allow_error)\n",
    "\n",
    "    def _process(self, r, allow_error = False):\n",
    "      if r.status_code == 500 or r.status_code == 403 or not allow_error:\n",
    "        print(r.text)\n",
    "        r.raise_for_status()\n",
    "      return r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21e65074-9d70-4bda-a80b-8ccc047e4126",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Extra: ligthning dataloader from hugging face dataset\n",
    "\n",
    "If your dataset is small, you could also load it from your spark dataframe with the huggingface dataset library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c25c14c-2d26-48f4-8840-06a928dfeb88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def define_lightning_dataset_moduel():\n",
    "\n",
    "  import pytorch_lightning as pl\n",
    "  from datasets import Dataset\n",
    "  class DeltaDataModuleHF(pl.LightningDataModule):\n",
    "      from torch.utils.data import random_split, DataLoader\n",
    "      def __init__(self, df, batch_size: int = 64):\n",
    "          super().__init__()\n",
    "          # For big dataset, you can use IterableDataset.from_spark()\n",
    "          self.dataset = Dataset.from_spark(df.select('content', 'label'))\n",
    "          self.splits = self.dataset.train_test_split(test_size=0.1)\n",
    "          self.batch_size = batch_size\n",
    "          self.transform = tf.Compose([\n",
    "                  tf.Lambda(lambda b: Image.open(io.BytesIO(b)).convert(\"RGB\")),\n",
    "                  tf.Resize(256),\n",
    "                  tf.CenterCrop(224),\n",
    "                  tf.ToTensor(),\n",
    "                  tf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "              ])\n",
    "          \n",
    "          self.train_ds = self.splits['train'].map(lambda e: {'content': self.transform(e['content']), 'label': e['label']})\n",
    "          self.train_ds.set_format(type='torch')\n",
    "          self.val_ds = self.splits['test'].map(lambda e: {'content': self.transform(e['content']), 'label': e['label']})\n",
    "          self.val_ds.set_format(type='torch')\n",
    "\n",
    "\n",
    "      def setup(self, stage: str):\n",
    "          print(f\"preparing dataset, stage: {stage}\")\n",
    "\n",
    "      def train_dataloader(self):\n",
    "          return torch.utils.data.DataLoader(self.train_ds, batch_size=self.batch_size, num_workers=8)\n",
    "        \n",
    "      def val_dataloader(self):\n",
    "          return torch.utils.data.DataLoader(self.val_ds, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "      def test_dataloader(self):\n",
    "          return torch.utils.data.DataLoader(self.val_ds, batch_size=self.batch_size, num_workers=8)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-init",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
