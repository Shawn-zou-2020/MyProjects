{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f3e2c9f-66a7-42f6-a514-2df403d35321",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Technical Setup notebook. Hide this cell results\n",
    "Initialize dataset to the current user and cleanup data when reset_all_data is set to true\n",
    "\n",
    "Do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72c85bcc-3859-4b48-9e10-7c06c72ea01f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "dbutils.widgets.text(\"min_dbr_version\", \"9.1\", \"Min required DBR version\")\n",
    "#Empty value will try default: dbdemos with a fallback to hive_metastore\n",
    "#Specifying a value will not have fallback and fail if the catalog can't be used/created\n",
    "dbutils.widgets.text(\"catalog\", \"\", \"Catalog\")\n",
    "#Empty value will be set to a database scoped to the current user using db_prefix\n",
    "dbutils.widgets.text(\"db\", \"\", \"Database\")\n",
    "#ignored if db is set (we force the databse to the given value in this case)\n",
    "dbutils.widgets.text(\"db_prefix\", \"\", \"Database prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8334dd23-e0fc-424e-bd5a-cb3ee67a0af2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pyspark.sql.functions import to_date, col, regexp_extract, rand, to_timestamp, initcap, sha1\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, input_file_name, col\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "# VERIFY DATABRICKS VERSION COMPATIBILITY ----------\n",
    "\n",
    "try:\n",
    "  min_required_version = dbutils.widgets.get(\"min_dbr_version\")\n",
    "except:\n",
    "  min_required_version = \"9.1\"\n",
    "\n",
    "version_tag = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\")\n",
    "version_search = re.search('^([0-9]*\\.[0-9]*)', version_tag)\n",
    "assert version_search, f\"The Databricks version can't be extracted from {version_tag}, shouldn't happen, please correct the regex\"\n",
    "current_version = float(version_search.group(1))\n",
    "assert float(current_version) >= float(min_required_version), f'The Databricks version of the cluster must be >= {min_required_version}. Current version detected: {current_version}'\n",
    "assert \"ml\" in version_tag.lower(), f\"The Databricks ML runtime must be used. Current version detected doesn't contain 'ml': {version_tag} \"\n",
    "\n",
    "\n",
    "#python Imports for ML...\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "from hyperopt import fmin, hp, tpe, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import SparkTrials\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import os\n",
    "import pandas as pd\n",
    "from hyperopt import space_eval\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#force the experiment to the field demos one. Required to launch as a batch\n",
    "def init_experiment_for_batch(demo_name, experiment_name):\n",
    "  #You can programatically get a PAT token with the following\n",
    "  pat_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "  url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "  headers = {\"Accept\": \"application/json\", \"Authorization\": f\"Bearer {pat_token}\"}\n",
    "  import requests\n",
    "  xp_root_path = f\"/Shared/dbdemos/experiments/{demo_name}\"\n",
    "  r = requests.post(f\"{url}/api/2.0/workspace/mkdirs\", headers = headers, json={ \"path\": xp_root_path})\n",
    "  if r.status_code != 200:\n",
    "    print(f\"ERROR: couldn't create a folder for the experiment under {xp_root_path} - please create the folder manually or  skip this init (used for job only: {r})\")\n",
    "  else:\n",
    "    for i in range(3):\n",
    "      #Wait to make sure the folder is created cause it's asynch?\n",
    "      folder = requests.get(f\"{url}/api/2.0/workspace/list\", headers = headers, params={ \"path\": xp_root_path}).json()\n",
    "      if folder.get('error_code', \"\") != 'RESOURCE_DOES_NOT_EXIST':\n",
    "        break\n",
    "    time.sleep(1*i)\n",
    "  xp = f\"{xp_root_path}/{experiment_name}\"\n",
    "  print(f\"Using common experiment under {xp}\")\n",
    "  mlflow.set_experiment(xp)\n",
    "  set_experiment_permission(xp)\n",
    "  return mlflow.get_experiment_by_name(xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36958cb2-bee9-4ea2-a847-1ffc204cefa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_cloud_name():\n",
    "  return spark.conf.get(\"spark.databricks.clusterUsageTags.cloudProvider\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b08ee61-2117-48eb-a4bc-8d4efc24ea02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.cloudFiles.schemaInference.sampleSize.numFiles\", \"10\")\n",
    "\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "if current_user.rfind('@') > 0:\n",
    "  current_user_no_at = current_user[:current_user.rfind('@')]\n",
    "else:\n",
    "  current_user_no_at = current_user\n",
    "current_user_no_at = re.sub(r'\\W+', '_', current_user_no_at)\n",
    "\n",
    "db = dbutils.widgets.get(\"db\")\n",
    "db_prefix = dbutils.widgets.get(\"db_prefix\")\n",
    "if len(db) == 0:\n",
    "  dbName = db_prefix+\"_\"+current_user_no_at\n",
    "else:\n",
    "  dbName = db\n",
    "\n",
    "\n",
    "#Try to use the UC catalog \"dbdemos\" when possible. IF not will fallback to hive_metastore\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "\n",
    "if len(db_prefix) > 0:\n",
    "    cloud_storage_path = f\"/Users/{current_user}/demos/{db_prefix}\"\n",
    "else:\n",
    "    cloud_storage_path = f\"/Users/{current_user}/demos/{catalog}_{db}\"\n",
    "    \n",
    "reset_all = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "\n",
    "\n",
    "def use_and_create_db(catalog, dbName, cloud_storage_path = None):\n",
    "  print(f\"USE CATALOG `{catalog}`\")\n",
    "  spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "  if cloud_storage_path is None or catalog not in ['hive_metastore', 'spark_catalog']:\n",
    "    spark.sql(f\"\"\"create database if not exists `{dbName}` \"\"\")\n",
    "  else:\n",
    "    spark.sql(f\"\"\"create database if not exists `{dbName}` LOCATION '{cloud_storage_path}/tables' \"\"\")\n",
    "\n",
    "if reset_all:\n",
    "  print(f'clearing up db {dbName} and cloud_storage_path={cloud_storage_path}')\n",
    "  spark.sql(f\"DROP DATABASE IF EXISTS `{dbName}` CASCADE\")\n",
    "  dbutils.fs.rm(cloud_storage_path, True)\n",
    "\n",
    "if catalog == \"spark_catalog\":\n",
    "  catalog = \"hive_metastore\"\n",
    "  \n",
    "#If the catalog is defined, we force it to the given value and throw exception if not.\n",
    "if len(catalog) > 0:\n",
    "  current_catalog = spark.sql(\"select current_catalog()\").collect()[0]['current_catalog()']\n",
    "  if current_catalog != catalog:\n",
    "    catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if catalog not in catalogs and catalog not in ['hive_metastore', 'spark_catalog']:\n",
    "      spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "      if catalog == \"dbdemos\":\n",
    "        spark.sql(f\"ALTER CATALOG {catalog} OWNER TO `account users`\")\n",
    "  use_and_create_db(catalog, dbName)\n",
    "else:\n",
    "  #otherwise we'll try to setup the catalog to DBDEMOS and create the database here. If we can't we'll fallback to legacy hive_metastore\n",
    "  print(\"Try to setup UC catalog\")\n",
    "  try:\n",
    "    catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if len(catalogs) == 1 and catalogs[0] in ['hive_metastore', 'spark_catalog']:\n",
    "      print(f\"UC doesn't appear to be enabled, will fallback to hive_metastore (spark_catalog)\")\n",
    "      catalog = \"hive_metastore\"\n",
    "    else:\n",
    "      if \"dbdemos\" not in catalogs:\n",
    "        spark.sql(\"CREATE CATALOG IF NOT EXISTS dbdemos\")\n",
    "        spark.sql(f\"ALTER CATALOG dbdemos OWNER TO `account users`\")\n",
    "      catalog = \"dbdemos\"\n",
    "    use_and_create_db(catalog, dbName)\n",
    "  except Exception as e:\n",
    "    print(f\"error with catalog {e}, do you have permission or UC enabled? will fallback to hive_metastore\")\n",
    "    catalog = \"hive_metastore\"\n",
    "    use_and_create_db(catalog, dbName)\n",
    "\n",
    "print(f\"using cloud_storage_path {cloud_storage_path}\")\n",
    "print(f\"using catalog.database `{catalog}`.`{dbName}`\")\n",
    "\n",
    "#Add the catalog to cloud storage path as we could have 1 checkpoint location different per catalog\n",
    "if catalog not in ['hive_metastore', 'spark_catalog']:\n",
    "  try:\n",
    "    if catalog == \"dbdemos\":\n",
    "      spark.sql(f\"GRANT CREATE, USAGE on DATABASE {catalog}.{dbName} TO `account users`\")\n",
    "      spark.sql(f\"ALTER SCHEMA {catalog}.{dbName} OWNER TO `account users`\")\n",
    "  except Exception as e:\n",
    "    print(\"Couldn't grant access to the schema to all users:\"+str(e))\n",
    "\n",
    "schema = dbName\n",
    "#with parallel execution this can fail the time of the initialization. add a few retry to fix these issues\n",
    "for i in range(10):\n",
    "  try:\n",
    "    spark.sql(f\"\"\"USE `{catalog}`.`{dbName}`\"\"\")\n",
    "    break\n",
    "  except Exception as e:\n",
    "    time.sleep(1)\n",
    "    if i >= 9:\n",
    "      raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2e4a1cd-4bc2-41cc-8344-b6d586248270",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_slide(slide_id, slide_number):\n",
    "  displayHTML(f'''\n",
    "  <div style=\"width:1150px; margin:auto\">\n",
    "  <iframe\n",
    "    src=\"https://docs.google.com/presentation/d/{slide_id}/embed?slide={slide_number}\"\n",
    "    frameborder=\"0\"\n",
    "    width=\"1150\"\n",
    "    height=\"683\"\n",
    "  ></iframe></div>\n",
    "  ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b94c1c31-578b-499b-9f16-2139a7c43ce2",
     "showTitle": true,
     "title": "Endpoint/model permission helper"
    }
   },
   "outputs": [],
   "source": [
    "def set_model_permission(model_name, permission, principal):\n",
    "  from databricks.sdk import WorkspaceClient\n",
    "  import databricks.sdk.service.catalog as c\n",
    "  sdk_client = WorkspaceClient()\n",
    "  sdk_client.registered_models.update(model_name, owner=principal)\n",
    "  return sdk_client.grants.update(c.SecurableType.FUNCTION, model_name, changes=[\n",
    "                            c.PermissionsChange(add=[c.Privilege[permission]], principal=principal)])\n",
    "\n",
    "def set_model_endpoint_permission(endpoint_name, permission, group_name):\n",
    "  from databricks.sdk import WorkspaceClient\n",
    "  import databricks.sdk.service.serving as s\n",
    "  sdk_client = WorkspaceClient()\n",
    "  ep = sdk_client.serving_endpoints.get(endpoint_name)\n",
    "  return sdk_client.serving_endpoints.set_permissions(serving_endpoint_id=ep.id, access_control_list=[s.ServingEndpointAccessControlRequest(permission_level=s.ServingEndpointPermissionLevel[permission], group_name=group_name)])\n",
    "\n",
    "def set_index_permission(index_name, permission, principal):\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    import databricks.sdk.service.catalog as c\n",
    "    sdk_client = WorkspaceClient()\n",
    "    return sdk_client.grants.update(c.SecurableType.TABLE, index_name, changes=[\n",
    "                            c.PermissionsChange(add=[c.Privilege[permission]], principal=principal)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "388691f5-397d-432e-9393-5ce203a75703",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_active_streams(start_with = \"\"):\n",
    "    return [s for s in spark.streams.active if len(start_with) == 0 or (s.name is not None and s.name.startswith(start_with))]\n",
    "  \n",
    "# Function to stop all streaming queries \n",
    "def stop_all_streams(start_with = \"\", sleep_time=0):\n",
    "  import time\n",
    "  time.sleep(sleep_time)\n",
    "  streams = get_active_streams(start_with)\n",
    "  if len(streams) > 0:\n",
    "    print(f\"Stopping {len(streams)} streams\")\n",
    "    for s in streams:\n",
    "        try:\n",
    "            s.stop()\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"All stream stopped {'' if len(start_with) == 0 else f'(starting with: {start_with}.)'}\")\n",
    "    \n",
    "def wait_for_all_stream(start = \"\"):\n",
    "  import time\n",
    "  actives = get_active_streams(start)\n",
    "  if len(actives) > 0:\n",
    "    print(f\"{len(actives)} streams still active, waiting... ({[s.name for s in actives]})\")\n",
    "  while len(actives) > 0:\n",
    "    spark.streams.awaitAnyTermination()\n",
    "    time.sleep(1)\n",
    "    actives = get_active_streams(start)\n",
    "  print(\"All streams completed.\")\n",
    "  \n",
    "def wait_for_table(table_name, timeout_duration=120):\n",
    "  import time\n",
    "  i = 0\n",
    "  while not spark._jsparkSession.catalog().tableExists(table_name) or spark.table(table_name).count() == 0:\n",
    "    time.sleep(1)\n",
    "    if i > timeout_duration:\n",
    "      raise Exception(f\"couldn't find table {table_name} or table is empty. Do you have data being generated to be consumed?\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1d6287d-38c8-47a4-b306-e0e41482f5de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import mlflow\n",
    "\n",
    "import databricks\n",
    "from datetime import datetime\n",
    "\n",
    "def get_automl_run(name):\n",
    "  #get the most recent automl run\n",
    "  df = spark.table(\"hive_metastore.dbdemos_metadata.automl_experiment\").filter(col(\"name\") == name).orderBy(col(\"date\").desc()).limit(1)\n",
    "  return df.collect()\n",
    "\n",
    "#Get the automl run information from the hive_metastore.dbdemos_metadata.automl_experiment table. \n",
    "#If it's not available in the metadata table, start a new run with the given parameters\n",
    "def get_automl_run_or_start(name, model_name, dataset, target_col, timeout_minutes, move_to_production = False):\n",
    "  spark.sql(\"create database if not exists hive_metastore.dbdemos_metadata\")\n",
    "  spark.sql(\"create table if not exists hive_metastore.dbdemos_metadata.automl_experiment (name string, date string)\")\n",
    "  result = get_automl_run(name)\n",
    "  if len(result) == 0:\n",
    "    print(\"No run available, start a new Auto ML run, this will take a few minutes...\")\n",
    "    start_automl_run(name, model_name, dataset, target_col, timeout_minutes, move_to_production)\n",
    "    return (False, get_automl_run(name))\n",
    "  return (True, result[0])\n",
    "\n",
    "\n",
    "#Start a new auto ml classification task and save it as metadata.\n",
    "def start_automl_run(name, model_name, dataset, target_col, timeout_minutes = 5, move_to_production = False):\n",
    "  from databricks import automl\n",
    "  automl_run = databricks.automl.classify(\n",
    "    dataset = dataset,\n",
    "    target_col = target_col,\n",
    "    timeout_minutes = timeout_minutes\n",
    "  )\n",
    "  experiment_id = automl_run.experiment.experiment_id\n",
    "  path = automl_run.experiment.name\n",
    "  data_run_id = mlflow.search_runs(experiment_ids=[automl_run.experiment.experiment_id], filter_string = \"tags.mlflow.source.name='Notebook: DataExploration'\").iloc[0].run_id\n",
    "  exploration_notebook_id = automl_run.experiment.tags[\"_databricks_automl.exploration_notebook_id\"]\n",
    "  best_trial_notebook_id = automl_run.experiment.tags[\"_databricks_automl.best_trial_notebook_id\"]\n",
    "\n",
    "  cols = [\"name\", \"date\", \"experiment_id\", \"experiment_path\", \"data_run_id\", \"best_trial_run_id\", \"exploration_notebook_id\", \"best_trial_notebook_id\"]\n",
    "  spark.createDataFrame(data=[(name, datetime.today().isoformat(), experiment_id, path, data_run_id, automl_run.best_trial.mlflow_run_id, exploration_notebook_id, best_trial_notebook_id)], schema = cols).write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"hive_metastore.dbdemos_metadata.automl_experiment\")\n",
    "  #Create & save the first model version in the MLFlow repo (required to setup hooks etc)\n",
    "  model_registered = mlflow.register_model(f\"runs:/{automl_run.best_trial.mlflow_run_id}/model\", model_name)\n",
    "  set_experiment_permission_automl(path)\n",
    "  if move_to_production:\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    print(\"registering model version \"+model_registered.version+\" as production model\")\n",
    "    client.transition_model_version_stage(name = model_name, version = model_registered.version, stage = \"Production\", archive_existing_versions=True)\n",
    "  return get_automl_run(name)\n",
    "\n",
    "#Generate nice link for the given auto ml run\n",
    "def display_automl_link(name, model_name, dataset, target_col, timeout_minutes = 5, move_to_production = False):\n",
    "  from_cache, r = get_automl_run_or_start(name, model_name, dataset, target_col, timeout_minutes, move_to_production)\n",
    "  if from_cache:\n",
    "    html = f\"\"\"For exploratory data analysis, open the <a href=\"/#notebook/{r[\"exploration_notebook_id\"]}\">data exploration notebook</a><br/><br/>\"\"\"\n",
    "    html += f\"\"\"To view the best performing model, open the <a href=\"/#notebook/{r[\"best_trial_notebook_id\"]}\">best trial notebook</a><br/><br/>\"\"\"\n",
    "    html += f\"\"\"To view details about all trials, navigate to the <a href=\"/#mlflow/experiments/{r[\"experiment_id\"]}/s?orderByKey=metrics.%60val_f1_score%60&orderByAsc=false\">MLflow experiment</>\"\"\"\n",
    "    displayHTML(html)\n",
    "\n",
    "def reset_automl_run(model_name):\n",
    "  if spark._jsparkSession.catalog().tableExists('hive_metastore.dbdemos_metadata.automl_experiment'):\n",
    "      spark.sql(f\"delete from hive_metastore.dbdemos_metadata.automl_experiment where name='{model_name}'\")\n",
    "\n",
    "def set_experiment_permission(experiment_path):\n",
    "  url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().extraContext().apply(\"api_url\")\n",
    "  import requests\n",
    "  pat_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "  headers =  {\"Authorization\": \"Bearer \" + pat_token, 'Content-type': 'application/json'}\n",
    "  status = requests.get(url+\"/api/2.0/workspace/get-status\", params = {\"path\": experiment_path}, headers=headers).json()\n",
    "  if \"object_id\" not in status:\n",
    "        print(f\"error setting up shared experiment permission: {status}\")\n",
    "  else:\n",
    "    #Set can manage to all users to the experiment we created as it's shared among all\n",
    "    params = {\"access_control_list\": [{\"group_name\": \"users\",\"permission_level\": \"CAN_MANAGE\"}]}\n",
    "    permissions = requests.patch(f\"{url}/api/2.0/permissions/experiments/{status['object_id']}\", json = params, headers=headers)\n",
    "    if permissions.status_code != 200:\n",
    "      print(\"ERROR: couldn't set permission to all users to the autoML experiment\")\n",
    "\n",
    "#Once the automl experiment is created, we assign CAN MANAGE to all users as it's shared in the workspace\n",
    "def set_experiment_permission_automl(experiment_path):\n",
    "  set_experiment_permission(experiment_path)\n",
    "  #try to find the experiment id\n",
    "  result = re.search(r\"_([a-f0-9]{8}_[a-f0-9]{4}_[a-f0-9]{4}_[a-f0-9]{4}_[a-f0-9]{12})_\", experiment_path)\n",
    "  if result is not None and len(result.groups()) > 0:\n",
    "    ex_id = result.group(0)\n",
    "  else:\n",
    "    print(experiment_path)\n",
    "    ex_id = experiment_path[experiment_path.rfind('/')+1:]\n",
    "\n",
    "  path = experiment_path\n",
    "  path = path[:path.rfind('/')]+\"/\"\n",
    "  #List to get the folder with the notebooks from the experiment\n",
    "  folders = requests.get(url+\"/api/2.0/workspace/list\", params = {\"path\": path}, headers=headers).json()\n",
    "  for f in folders['objects']:\n",
    "    if f['object_type'] == 'DIRECTORY' and ex_id in f['path']:\n",
    "        #Set the permission of the experiment notebooks to all\n",
    "        permissions = requests.patch(f\"{url}/api/2.0/permissions/directories/{f['object_id']}\", json = params, headers=headers)\n",
    "        if permissions.status_code != 200:\n",
    "          print(\"ERROR: couldn't set permission to all users to the autoML experiment notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9ad2e41-1436-4dba-a707-050a291fefe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_fs_table(table_name):\n",
    "  try:\n",
    "    #drop table if exists\n",
    "    fs.drop_table(table_name)\n",
    "    spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e58b0d4-9780-421e-bfbb-240a4d653870",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Deprecated. TODO: remove all occurence with is_folder_empty\n",
    "def test_not_empty_folder(folder):\n",
    "  try:\n",
    "    return len(dbutils.fs.ls(folder)) > 0\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "#Return true if the folder is empty or does not exists\n",
    "def is_folder_empty(folder):\n",
    "  try:\n",
    "    return len(dbutils.fs.ls(folder)) == 0\n",
    "  except:\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82006720-10b5-4490-91fb-08488bebd354",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', SyntaxWarning)\n",
    "    warnings.simplefilter('ignore', DeprecationWarning)\n",
    "    warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02ba4836-c546-4729-94c4-6272bf27462c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import collections\n",
    "import os\n",
    "def download_file(url, destination):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        print('saving '+destination+'/'+local_filename)\n",
    "        with open(destination+'/'+local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "  \n",
    "def download_file_from_git(dest, owner, repo, path):\n",
    "    if not os.path.exists(dest):\n",
    "      os.makedirs(dest)\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    files = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents{path}').json()\n",
    "    files = [f['download_url'] for f in files if 'NOTICE' not in f['name']]\n",
    "    def download_to_dest(url):\n",
    "         download_file(url, dest)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        collections.deque(executor.map(download_to_dest, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c59f5c38-7290-49dc-a018-72ac4a10efc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_last_experiment(demo_name, experiment_path = \"/Shared/dbdemos/experiments/\"):\n",
    "    import requests\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    #TODO: waiting for https://github.com/databricks/databricks-sdk-py/issues/509 to use the python sdk instead\n",
    "    base_url =dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    r = requests.get(base_url+\"/api/2.0/workspace/list\", params={'path': f\"{experiment_path}/{demo_name}\"}, headers=headers).json()\n",
    "    xps = [f for f in r['objects'] if f['object_type'] == 'MLFLOW_EXPERIMENT' and 'automl_churn' in f['path']]\n",
    "    xps = [x for x in xps if re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})', x['path'])]\n",
    "    sorted_xp = sorted(xps, key=lambda f: f['path'], reverse = True)\n",
    "    print(len(sorted_xp))\n",
    "    if len(sorted_xp) == 0:\n",
    "        raise Exception(f\"No experiment available for this demo. Please re-run the previous notebook with the AutoML run. - {r}\")\n",
    "\n",
    "    last_xp = sorted_xp[0]\n",
    "\n",
    "    # Search for the date pattern in the input string\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})', last_xp['path'])\n",
    "\n",
    "    if match:\n",
    "        date_str = match.group(1)  # Extract the matched date string\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d_%H:%M:%S')  # Convert to a datetime object\n",
    "        # Calculate the difference in days from the current date\n",
    "        days_difference = (datetime.now() - date).days\n",
    "        if days_difference > 30:\n",
    "            raise Exception(f\"It looks like the last experiment {last_xp} is too old ({days} days). Please re-run the previous notebook to make sure you have the latest version.\")\n",
    "    else:\n",
    "        raise Exception(f\"Invalid experiment format or no experiment available. Please re-run the previous notebook. {last_xp['path']}\")\n",
    "    return last_xp"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00-global-setup",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
