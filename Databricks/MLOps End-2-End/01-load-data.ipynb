{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6a808ed-b4c1-4704-9694-51dab8ed43ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data initialization notebook. \n",
    "Do not run outside of the main notebook. This will automatically be called based on the reste_all widget value to setup the data required for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7662ca5a-7cc4-4d26-8424-e1b54b0314a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb24f1df-a2a1-45e7-b1ae-104535c44c3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.widgets.text(\"raw_data_location\", \"/demos/retail/churn/\", \"Raw data location (stating dir)\")\n",
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "\n",
    "def cleanup_folder(path):\n",
    "  #Cleanup to have something nicer\n",
    "  for f in dbutils.fs.ls(path):\n",
    "    if f.name.startswith('_committed') or f.name.startswith('_started') or f.name.startswith('_SUCCESS') :\n",
    "      dbutils.fs.rm(f.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30d27ec6-2d9f-4fbd-a05e-fffa2ab14d9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder = \"/demos/retail/churn\"\n",
    "if reset_all_data:\n",
    "  print(\"resetting all data...\")\n",
    "  if folder.count('/') > 2 and folder.startswith('/demos'):\n",
    "    dbutils.fs.rm(folder, True)\n",
    "\n",
    "data_exists = False\n",
    "try:\n",
    "  dbutils.fs.ls(folder)\n",
    "  dbutils.fs.ls(folder+\"/orders\")\n",
    "  dbutils.fs.ls(folder+\"/users\")\n",
    "  dbutils.fs.ls(folder+\"/events\")\n",
    "  dbutils.fs.ls(folder+\"/ml_features\")\n",
    "  data_exists = True\n",
    "  print(\"data already exists\")\n",
    "except:\n",
    "  print(\"folder doesn't exists, generating the data...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e71a3ee-beed-46b7-a44a-9bf67af0016d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Let's download the data from dbdemos resource repo\n",
    "If this fails, fallback on generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde99bac-4315-45f4-89a5-a243a2982766",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import collections\n",
    "def download_file(url, destination):\n",
    "    if not os.path.exists(destination):\n",
    "      os.makedirs(destination)\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        print('saving '+destination+'/'+local_filename)\n",
    "        with open(destination+'/'+local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "  \n",
    "def download_file_from_git(dest, owner, repo, path):\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    files = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents{path}').json()\n",
    "    files = [f['download_url'] for f in files if 'NOTICE' not in f['name']]\n",
    "    def download_to_dest(url):\n",
    "         download_file(url, dest)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        collections.deque(executor.map(download_to_dest, files))\n",
    "\n",
    "data_downloaded = False\n",
    "if not data_exists:\n",
    "    try:\n",
    "        download_file_from_git('/dbfs'+folder+'/events', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/events\")\n",
    "        download_file_from_git('/dbfs'+folder+'/orders', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/orders\")\n",
    "        download_file_from_git('/dbfs'+folder+'/users', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/users\")\n",
    "        download_file_from_git('/dbfs'+folder+'/ml_features', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/ml_features\")\n",
    "        data_downloaded = True\n",
    "    except Exception as e: \n",
    "        print(f\"Error trying to download the file from the repo: {str(e)}. Will generate the data instead...\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1567896-56f9-480b-b4ea-578e3c15d7f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#As we need a model in the DLT pipeline and the model depends of the DLT pipeline too, let's build an empty one.\n",
    "#This wouldn't make sense in a real-world system where we'd have 2 jobs / pipeline (1 for ingestion, and 1 to build the model / run inferences)\n",
    "import random\n",
    "import mlflow\n",
    "from  mlflow.models.signature import ModelSignature\n",
    "\n",
    "# define a custom model\n",
    "class ChurnEmptyModel(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input):\n",
    "        import random\n",
    "        return model_input['user_id'].apply(lambda x: random.randint(0, 1)).astype('int32')\n",
    "\n",
    "model_name = \"dbdemos_customer_churn\"\n",
    "#Only register empty model if model doesn't exist yet\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "try:\n",
    "    client.get_registered_model(name=model_name)\n",
    "except Exception as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        print(\"Model doesn't exist - saving an empty one\")\n",
    "        # save the model\n",
    "        churn_model = ChurnEmptyModel()\n",
    "        import pandas as pd\n",
    "        signature = ModelSignature.from_dict({'inputs': '[{\"name\": \"user_id\", \"type\": \"string\"}, {\"name\": \"age_group\", \"type\": \"integer\"}, {\"name\": \"canal\", \"type\": \"string\"}, {\"name\": \"country\", \"type\": \"string\"}, {\"name\": \"gender\", \"type\": \"integer\"}, {\"name\": \"order_count\", \"type\": \"long\"}, {\"name\": \"total_amount\", \"type\": \"long\"}, {\"name\": \"total_item\", \"type\": \"long\"}, {\"name\": \"last_transaction\", \"type\": \"datetime\"}, {\"name\": \"platform\", \"type\": \"string\"}, {\"name\": \"event_count\", \"type\": \"long\"}, {\"name\": \"session_count\", \"type\": \"long\"}, {\"name\": \"days_since_creation\", \"type\": \"integer\"}, {\"name\": \"days_since_last_activity\", \"type\": \"integer\"}, {\"name\": \"days_last_event\", \"type\": \"integer\"}]',\n",
    "        'outputs': '[{\"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"int32\", \"shape\": [-1]}}]'})\n",
    "        with mlflow.start_run() as run:\n",
    "            model_info = mlflow.pyfunc.log_model(artifact_path=\"model\", python_model=churn_model, signature=signature)\n",
    "\n",
    "        model_registered = mlflow.register_model(f\"runs:/{ run.info.run_id }/model\", model_name)\n",
    "        #Move the model in production\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        client.transition_model_version_stage(model_name, model_registered.version, stage = \"Production\", archive_existing_versions=True)\n",
    "    else:\n",
    "        print(f\"ERROR: couldn't access model for unknown reason - DLT pipeline will likely fail as model isn't available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08b561aa-aa6d-45e5-9c9e-ac18cc619d22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if data_downloaded:\n",
    "  dbutils.notebook.exit(\"data downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "413d8737-0a65-435f-89b3-7b158de5893f",
     "showTitle": true,
     "title": "users data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from faker import Faker\n",
    "from collections import OrderedDict \n",
    "import uuid\n",
    "fake = Faker()\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "fake_firstname = F.udf(fake.first_name)\n",
    "fake_lastname = F.udf(fake.last_name)\n",
    "fake_email = F.udf(fake.ascii_company_email)\n",
    "\n",
    "def fake_date_between(months=0):\n",
    "  start = datetime.now() - timedelta(days=30*months)\n",
    "  return F.udf(lambda: fake.date_between_dates(date_start=start, date_end=start + timedelta(days=30)).strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "\n",
    "fake_date = F.udf(lambda:fake.date_time_this_month().strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "fake_date_old = F.udf(lambda:fake.date_between_dates(date_start=datetime(2012,1,1), date_end=datetime(2015,12,31)).strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "fake_address = F.udf(fake.address)\n",
    "canal = OrderedDict([(\"WEBAPP\", 0.5),(\"MOBILE\", 0.1),(\"PHONE\", 0.3),(None, 0.01)])\n",
    "fake_canal = F.udf(lambda:fake.random_elements(elements=canal, length=1)[0])\n",
    "fake_id = F.udf(lambda: str(uuid.uuid4()) if random.uniform(0, 1) < 0.98 else None)\n",
    "countries = ['FR', 'USA', 'SPAIN']\n",
    "fake_country = F.udf(lambda: countries[random.randint(0,2)])\n",
    "\n",
    "def get_df(size, month):\n",
    "  df = spark.range(0, size).repartition(10)\n",
    "  df = df.withColumn(\"id\", fake_id())\n",
    "  df = df.withColumn(\"firstname\", fake_firstname())\n",
    "  df = df.withColumn(\"lastname\", fake_lastname())\n",
    "  df = df.withColumn(\"email\", fake_email())\n",
    "  df = df.withColumn(\"address\", fake_address())\n",
    "  df = df.withColumn(\"canal\", fake_canal())\n",
    "  df = df.withColumn(\"country\", fake_country())  \n",
    "  df = df.withColumn(\"creation_date\", fake_date_between(month)())\n",
    "  df = df.withColumn(\"last_activity_date\", fake_date())\n",
    "  df = df.withColumn(\"gender\", F.round(F.rand()+0.2))\n",
    "  return df.withColumn(\"age_group\", F.round(F.rand()*10))\n",
    "\n",
    "df_customers = get_df(133, 12*30).withColumn(\"creation_date\", fake_date_old())\n",
    "for i in range(1, 24):\n",
    "  df_customers = df_customers.union(get_df(2000+i*200, 24-i))\n",
    "\n",
    "df_customers = df_customers.cache()\n",
    "\n",
    "ids = df_customers.select(\"id\").collect()\n",
    "ids = [r[\"id\"] for r in ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "440a26f1-28c2-4220-8f7d-e5b1f0377253",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Number of order per customer to generate a nicely distributed dataset\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "mu, sigma = 3, 2 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, int(len(ids)))\n",
    "s = [i if i > 0 else 0 for i in s]\n",
    "\n",
    "#Most of our customers have ~3 orders\n",
    "import matplotlib.pyplot as plt\n",
    "count, bins, ignored = plt.hist(s, 30, density=False)\n",
    "plt.show()\n",
    "s = [int(i) for i in s]\n",
    "\n",
    "order_user_ids = list()\n",
    "action_user_ids = list()\n",
    "for i, id in enumerate(ids):\n",
    "  for j in range(1, s[i]):\n",
    "    order_user_ids.append(id)\n",
    "    #Let's make 5 more actions per order (5 click on the website to buy something)\n",
    "    for j in range(1, 5):\n",
    "      action_user_ids.append(id)\n",
    "      \n",
    "print(f\"Generated {len(order_user_ids)} orders and  {len(action_user_ids)} actions for {len(ids)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29e5a662-83c9-4982-bdf3-039ab8fc0d49",
     "showTitle": true,
     "title": "order data"
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.createDataFrame([(i,) for i in order_user_ids], ['user_id'])\n",
    "orders = orders.withColumn(\"id\", fake_id())\n",
    "orders = orders.withColumn(\"transaction_date\", fake_date())\n",
    "orders = orders.withColumn(\"item_count\", F.round(F.rand()*2)+1)\n",
    "orders = orders.withColumn(\"amount\", F.col(\"item_count\")*F.round(F.rand()*30+10))\n",
    "orders = orders.cache()\n",
    "orders.repartition(10).write.format(\"json\").mode(\"overwrite\").save(folder+\"/orders\")\n",
    "cleanup_folder(folder+\"/orders\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcc1fde3-f9ad-4660-a9cd-c2683722555f",
     "showTitle": true,
     "title": "website actions"
    }
   },
   "outputs": [],
   "source": [
    "#Website interaction\n",
    "import re\n",
    "\n",
    "platform = OrderedDict([(\"ios\", 0.5),(\"android\", 0.1),(\"other\", 0.3),(None, 0.01)])\n",
    "fake_platform = F.udf(lambda:fake.random_elements(elements=platform, length=1)[0])\n",
    "\n",
    "action_type = OrderedDict([(\"view\", 0.5),(\"log\", 0.1),(\"click\", 0.3),(None, 0.01)])\n",
    "fake_action = F.udf(lambda:fake.random_elements(elements=action_type, length=1)[0])\n",
    "fake_uri = F.udf(lambda:re.sub(r'https?:\\/\\/.*?\\/', \"https://databricks.com/\", fake.uri()))\n",
    "\n",
    "\n",
    "actions = spark.createDataFrame([(i,) for i in order_user_ids], ['user_id']).repartition(20)\n",
    "actions = actions.withColumn(\"event_id\", fake_id())\n",
    "actions = actions.withColumn(\"platform\", fake_platform())\n",
    "actions = actions.withColumn(\"date\", fake_date())\n",
    "actions = actions.withColumn(\"action\", fake_action())\n",
    "actions = actions.withColumn(\"session_id\", fake_id())\n",
    "actions = actions.withColumn(\"url\", fake_uri())\n",
    "actions = actions.cache()\n",
    "actions.write.format(\"csv\").option(\"header\", True).mode(\"overwrite\").save(folder+\"/events\")\n",
    "cleanup_folder(folder+\"/events\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "902339c5-aef7-4c62-8f4b-cbd3718f0249",
     "showTitle": true,
     "title": "Compute churn and save users"
    }
   },
   "outputs": [],
   "source": [
    "#Let's generate the Churn information. We'll fake it based on the existing data & let our ML model learn it\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "churn_proba_action = actions.groupBy('user_id').agg({'platform': 'first', '*': 'count'}).withColumnRenamed(\"count(1)\", \"action_count\")\n",
    "#Let's count how many order we have per customer.\n",
    "churn_proba = orders.groupBy('user_id').agg({'item_count': 'sum', '*': 'count'})\n",
    "churn_proba = churn_proba.join(churn_proba_action, ['user_id'])\n",
    "churn_proba = churn_proba.join(df_customers, churn_proba.user_id == df_customers.id)\n",
    "\n",
    "#Customer having > 5 orders are likely to churn\n",
    "\n",
    "churn_proba = (churn_proba.withColumn(\"churn_proba\", 5 +  F.when(((col(\"count(1)\") >=5) & (col(\"first(platform)\") == \"ios\")) |\n",
    "                                                                 ((col(\"count(1)\") ==3) & (col(\"gender\") == 0)) |\n",
    "                                                                 ((col(\"count(1)\") ==2) & (col(\"gender\") == 1) & (col(\"age_group\") <= 3)) |\n",
    "                                                                 ((col(\"sum(item_count)\") <=1) & (col(\"first(platform)\") == \"android\")) |\n",
    "                                                                 ((col(\"sum(item_count)\") >=10) & (col(\"first(platform)\") == \"ios\")) |\n",
    "                                                                 (col(\"action_count\") >=4) |\n",
    "                                                                 (col(\"country\") == \"USA\") |\n",
    "                                                                 ((F.datediff(F.current_timestamp(), col(\"creation_date\")) >= 90)) |\n",
    "                                                                 ((col(\"age_group\") >= 7) & (col(\"gender\") == 0)) |\n",
    "                                                                 ((col(\"age_group\") <= 2) & (col(\"gender\") == 1)), 80).otherwise(20)))\n",
    "\n",
    "churn_proba = churn_proba.withColumn(\"churn\", F.rand()*100 < col(\"churn_proba\"))\n",
    "churn_proba = churn_proba.drop(\"user_id\", \"churn_proba\", \"sum(item_count)\", \"count(1)\", \"first(platform)\", \"action_count\")\n",
    "churn_proba.repartition(100).write.format(\"json\").mode(\"overwrite\").save(folder+\"/users\")\n",
    "cleanup_folder(folder+\"/users\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-load-data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
