{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12fa34f6-3be5-4a98-8cf0-2b8798cbf10c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create churn table\n",
    "\n",
    "Create tables in the given catalog (uc or legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5561666c-6b87-448c-8b6c-5d0c04507630",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "dbutils.widgets.text(\"root_folder\", \"lakehouse-retail-c360\", \"Demo root folder\")\n",
    "dbutils.widgets.text(\"catalog\", \"\", \"UC catalog\")\n",
    "dbutils.widgets.text(\"db\", \"\", \"Database\")\n",
    "\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "root_folder = dbutils.widgets.get(\"root_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45b2ef53-4db9-446b-b0be-e472e7a9608d",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-global-setup $reset_all_data=$reset_all_data $db_prefix=retail $catalog=$catalog $db=$db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1caae29-7263-4756-aeac-eee8e09046c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "fs = FeatureStoreClient()\n",
    "database = dbName\n",
    "def display_automl_churn_link(dataset, model_name, force_refresh = False): \n",
    "  if force_refresh:\n",
    "    reset_automl_run(\"lakehouse_churn_auto_ml\")\n",
    "  display_automl_link(\"lakehouse_churn_auto_ml\", model_name, dataset, \"churn\", 5, move_to_production=False)\n",
    "\n",
    "def get_automl_churn_run(force_refresh = False): \n",
    "  if force_refresh:\n",
    "    reset_automl_run(\"lakehouse_churn_auto_ml\")\n",
    "  from_cache, r = get_automl_run_or_start(\"lakehouse_churn_auto_ml\", \"dbdemos_customer_churn\", fs.read_table(f'{database}.churn_user_features'), \"churn\", 5, move_to_production=False)\n",
    "  return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2049767b-fd83-4d2e-83d3-6a6d2467623c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, sha1, col, initcap, to_timestamp\n",
    "\n",
    "raw_data_location = cloud_storage_path\n",
    "folder = \"/demos/retail/churn\"\n",
    "\n",
    "if reset_all_data or is_folder_empty(folder+\"/orders\") or is_folder_empty(folder+\"/users\") or is_folder_empty(folder+\"/events\"):\n",
    "  #data generation on another notebook to avoid installing libraries (takes a few seconds to setup pip env)\n",
    "  print(f\"Generating data under {folder} , please wait a few sec...\")\n",
    "  path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "  parent_count = path[path.rfind(root_folder):].count('/') - 1\n",
    "  prefix = \"./\" if parent_count == 0 else parent_count*\"../\"\n",
    "  prefix = f'{prefix}_resources/'\n",
    "  dbutils.notebook.run(prefix+\"01-load-data\", 600)\n",
    "else:\n",
    "  print(\"data already existing. Run with reset_all_data=true to force a data cleanup for your local demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98c4a1b3-c05f-434e-9def-f8d676e20b21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_folder(folder, data_format, table, hints = None):\n",
    "  bronze_products = (spark.readStream\n",
    "                              .format(\"cloudFiles\")\n",
    "                              .option(\"cloudFiles.format\", data_format)\n",
    "                              .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                              .option(\"cloudFiles.schemaHints\", None)\n",
    "                              .option(\"cloudFiles.schemaLocation\", f\"{raw_data_location}/schema/{table}\") #Autoloader will automatically infer all the schema & evolution\n",
    "                              .load(folder))\n",
    "\n",
    "  return (bronze_products.writeStream\n",
    "                    .option(\"checkpointLocation\", f\"{raw_data_location}/checkpoint/{table}\") #exactly once delivery on Delta tables over restart/kill\n",
    "                    .option(\"mergeSchema\", \"true\") #merge any new column dynamically\n",
    "                    .trigger(once = True) #Remove for real time streaming\n",
    "                    .table(table))\n",
    "\n",
    "if not spark._jsparkSession.catalog().tableExists(f\"`{catalog}`.`{database}`.`churn_orders_bronze`\") or \\\n",
    "   not spark._jsparkSession.catalog().tableExists(f\"`{catalog}`.`{database}`.`churn_app_events`\") or \\\n",
    "   not spark._jsparkSession.catalog().tableExists(f\"`{catalog}`.`{database}`.`churn_users_bronze`\") or \\\n",
    "   not spark._jsparkSession.catalog().tableExists(f\"`{catalog}`.`{database}`.`churn_features`\"):  \n",
    "  #One of the table is missing, let's rebuild them all\n",
    "  spark.sql(f\"drop table if exists `{catalog}`.`{database}`.`churn_orders_bronze`\")\n",
    "  spark.sql(f\"drop table if exists `{catalog}`.`{database}`.`churn_app_events`\")\n",
    "  spark.sql(f\"drop table if exists `{catalog}`.`{database}`.`churn_users_bronze`\")\n",
    "  spark.sql(f\"drop table if exists `{catalog}`.`{database}`.`churn_features`\")\n",
    "  #drop the checkpoints \n",
    "  if raw_data_location.count('/') > 3:\n",
    "    dbutils.fs.rm(raw_data_location, True)  \n",
    "  qo = ingest_folder('/demos/retail/churn/orders', 'json', 'churn_orders_bronze')\n",
    "  qe = ingest_folder('/demos/retail/churn/events', 'csv', 'churn_app_events')\n",
    "  qu = ingest_folder('/demos/retail/churn/users', 'json',  'churn_users_bronze')\n",
    "\n",
    "  qo.awaitTermination()\n",
    "  qe.awaitTermination()\n",
    "  qu.awaitTermination()\n",
    "\n",
    "  q = (spark.readStream \n",
    "          .table(\"churn_users_bronze\")\n",
    "            .withColumnRenamed(\"id\", \"user_id\")\n",
    "            .withColumn(\"email\", sha1(col(\"email\")))\n",
    "            .withColumn(\"creation_date\", to_timestamp(col(\"creation_date\"), \"MM-dd-yyyy H:mm:ss\"))\n",
    "            .withColumn(\"last_activity_date\", to_timestamp(col(\"last_activity_date\"), \"MM-dd-yyyy HH:mm:ss\"))\n",
    "            .withColumn(\"firstname\", initcap(col(\"firstname\")))\n",
    "            .withColumn(\"lastname\", initcap(col(\"lastname\")))\n",
    "            .withColumn(\"age_group\", col(\"age_group\").cast('int'))\n",
    "            .withColumn(\"gender\", col(\"gender\").cast('int'))\n",
    "            .withColumn(\"churn\", col(\"churn\").cast('int'))\n",
    "            .drop(col(\"_rescued_data\"))\n",
    "       .writeStream\n",
    "          .option(\"checkpointLocation\", f\"{raw_data_location}/checkpoint/users\")\n",
    "          .trigger(once=True)\n",
    "          .table(\"churn_users\"))\n",
    "\n",
    "\n",
    "  (spark.readStream \n",
    "          .table(\"churn_orders_bronze\")\n",
    "            .withColumnRenamed(\"id\", \"order_id\")\n",
    "            .withColumn(\"amount\", col(\"amount\").cast('int'))\n",
    "            .withColumn(\"item_count\", col(\"item_count\").cast('int'))\n",
    "            .withColumn(\"creation_date\", to_timestamp(col(\"transaction_date\"), \"MM-dd-yyyy H:mm:ss\"))\n",
    "            .drop(col(\"_rescued_data\"))\n",
    "       .writeStream\n",
    "          .option(\"checkpointLocation\", f\"{raw_data_location}/checkpoint/orders\")\n",
    "          .trigger(once=True)\n",
    "          .table(\"churn_orders\")).awaitTermination()\n",
    "  q.awaitTermination()\n",
    "\n",
    "  spark.sql(\"\"\"\n",
    "      CREATE OR REPLACE TABLE churn_features AS\n",
    "      WITH \n",
    "          churn_orders_stats AS (SELECT user_id, count(*) as order_count, sum(amount) as total_amount, sum(item_count) as total_item, max(creation_date) as last_transaction\n",
    "            FROM churn_orders GROUP BY user_id),  \n",
    "          churn_app_events_stats as (\n",
    "            SELECT first(platform) as platform, user_id, count(*) as event_count, count(distinct session_id) as session_count, max(to_timestamp(date, \"MM-dd-yyyy HH:mm:ss\")) as last_event\n",
    "              FROM churn_app_events GROUP BY user_id)\n",
    "        SELECT *, \n",
    "           datediff(now(), creation_date) as days_since_creation,\n",
    "           datediff(now(), last_activity_date) as days_since_last_activity,\n",
    "           datediff(now(), last_event) as days_last_event\n",
    "           FROM churn_users\n",
    "             INNER JOIN churn_orders_stats using (user_id)\n",
    "             INNER JOIN churn_app_events_stats using (user_id) \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-create-churn-tables",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
